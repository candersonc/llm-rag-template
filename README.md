# LLM-RAG Template üöÄ

A production-ready template for building RAG-powered document analysis systems with ChromaDB and LLM integration.

## ‚ú® Features

- **üöÄ Instant Setup** - Clone and run in under 2 minutes
- **üíæ ChromaDB Powered** - Fast, persistent vector search with metadata filtering
- **üìÑ Any Document Type** - PDF, TXT, MD, HTML, DOCX, JSON support
- **üîí Local LLM** - Privacy-first with Ollama (easily swap to OpenAI/Anthropic)
- **üí¨ Interactive Demo** - User-friendly command-line interface
- **üè≠ Production Ready** - Logging, error handling, extensible architecture

## üéØ Use Cases

- **Legal**: Contract analysis, compliance checking, legal research
- **Medical**: Clinical document processing, medical literature review
- **Financial**: Report analysis, invoice processing, audit documentation
- **Research**: Literature review, knowledge extraction, data analysis
- **Support**: Documentation Q&A, helpdesk automation, knowledge base

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai) (for local LLM)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/llm-rag-template.git
cd llm-rag-template

# 2. Install dependencies
pip install -r requirements.txt

# 3. Install Ollama (if not already installed)
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# 4. Add your documents (optional - demo docs provided)
cp your-documents/* documents/

# 5. Run the interactive demo
./run.sh
```

### First Run

When you run `./run.sh` for the first time:

1. The script will check all dependencies
2. Download the LLM model (Llama 3.2) if needed
3. Create sample documents if none exist
4. Build the vector database index
5. Start the interactive query interface

```
‚ùì What would you like to know about your documents? > What are the main topics covered?

üîç Searching documents for relevant information...

üí° Answer:
----------
Based on the documents, the main topics covered are:
1. RAG (Retrieval-Augmented Generation) systems and how they work
2. Vector databases comparison (ChromaDB, FAISS, Pinecone, etc.)
3. Best practices for LLM applications including prompt engineering and chunking strategies
----------
```

## üìÅ Project Structure

```
llm-rag-template/
‚îú‚îÄ‚îÄ run.sh              # üöÄ Interactive demo launcher
‚îú‚îÄ‚îÄ processor.py        # üîç RAG query processor
‚îú‚îÄ‚îÄ indexer.py         # üìö Document indexer
‚îú‚îÄ‚îÄ config.py          # ‚öôÔ∏è  Configuration settings
‚îú‚îÄ‚îÄ requirements.txt   # üì¶ Python dependencies
‚îú‚îÄ‚îÄ documents/         # üìÑ Your documents go here
‚îú‚îÄ‚îÄ chroma_db/         # üíæ Vector database storage
‚îú‚îÄ‚îÄ output/           # üìä Query results and logs
‚îî‚îÄ‚îÄ examples/         # üí° Example implementations
    ‚îú‚îÄ‚îÄ legal/        # Legal document processing
    ‚îú‚îÄ‚îÄ medical/      # Medical records analysis
    ‚îî‚îÄ‚îÄ financial/    # Financial document analysis
```

## üîß Configuration

Edit `config.py` to customize:

```python
# Change embedding model
EMBEDDING_MODEL = "all-mpnet-base-v2"  # Better quality

# Change LLM model
LLM_MODEL = "mistral:7b"  # Alternative model

# Adjust chunking
CHUNK_SIZE = 1500  # Larger chunks
CHUNK_OVERLAP = 300  # More overlap

# Tune retrieval
TOP_K_RESULTS = 10  # Retrieve more context
```

## üé® Customization

The template includes `TODO: CUSTOMIZE` markers at all key extension points:

### 1. Add New Document Types

In `indexer.py`:
```python
def parse_document(self, file_path: Path) -> Dict:
    # TODO: CUSTOMIZE - Add parsing logic for different formats
    if file_path.suffix == '.csv':
        # Add CSV parsing
        import pandas as pd
        df = pd.read_csv(file_path)
        content = df.to_string()
```

### 2. Change LLM Provider

In `processor.py`:
```python
def query_llm(self, query: str, context: str) -> str:
    # TODO: CUSTOMIZE - Add your LLM query logic

    # For OpenAI:
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
```

### 3. Modify Output Format

In `run.sh`:
```bash
# TODO: OUTPUT/CUSTOMIZE - This is where you customize the output format
# Save as JSON
echo "{\"query\": \"$user_query\", \"response\": \"$response\"}" > output.json

# Send to API
curl -X POST https://your-api.com/results -d "{\"response\": \"$response\"}"
```

## üß™ Testing

Test with your own documents:

```bash
# Add test documents
echo "Your content here" > documents/test.txt

# Rebuild index
./run.sh
> index

# Query
> What does the test document contain?
```

## üö¢ Deployment

### Docker

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["python", "processor.py"]
```

### Cloud Functions

The template can be deployed to:
- AWS Lambda
- Google Cloud Run
- Azure Functions
- Vercel Functions

## üìä Performance

- **Indexing**: ~1000 documents/minute
- **Query Speed**: 2-5 seconds per query
- **Memory Usage**: ~500MB for 10k documents
- **Accuracy**: Depends on embedding model and chunk size

## üõ†Ô∏è Troubleshooting

### Common Issues

1. **"Ollama not found"**
   ```bash
   # Install Ollama
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **"No module named chromadb"**
   ```bash
   pip install chromadb sentence-transformers
   ```

3. **"Connection refused" error**
   ```bash
   # Start Ollama service
   ollama serve
   ```

4. **Slow indexing**
   - Reduce chunk size in `config.py`
   - Use smaller embedding model

## ü§ù Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìö Resources

- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Sentence Transformers](https://www.sbert.net/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)

## üìÑ License

MIT License - Use this template for any purpose!

## ‚≠ê Star Us!

If this template helps you build something awesome, please star the repo!

---

Built with ‚ù§Ô∏è for the RAG community